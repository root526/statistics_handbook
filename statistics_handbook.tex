\documentclass[11pt]{ltjsarticle}
% \usepackage{luatexja}
\usepackage[top=30truemm,bottom=30truemm,left=25truemm,right=25truemm]{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
  \theoremstyle{definition}
  \newtheorem{theorem}{定理}[section]
  \newtheorem{definition}[theorem]{定義}
  \newtheorem{lemma}[theorem]{補題}
  \newtheorem{corollary}[theorem]{系}
  \newtheorem{proposition}[theorem]{命題}
  \newtheorem*{remark}{注意}
  \newtheorem{example}{例}
\usepackage{amssymb}
\usepackage[stable]{footmisc}
% \usepackage[all]{xy}
% \usepackage{graphicx}
%\begin{figure}[t]
%  \begin{center}
%    \includegraphics[width=\linewidth,pagebox=cropbox,clip]{}
%    \caption{}
%    \label{}
% \end{center}
%\end{figure}
\usepackage{here}
\usepackage{enumerate}%\begin{enumerate}[括弧の種類]
\usepackage{mathrsfs}%\mathscr{}
\usepackage{braket} %\bra{}, \ket{}, \braket{ | | }
\usepackage{bm}
\usepackage[
    luatex,
    pdfencoding=auto,
    colorlinks=true,
    linkcolor=blue,
    setpagesize=false,
    bookmarks=true,
    bookmarksnumbered=true
]{hyperref}
\usepackage{mathtools}

%color
\usepackage{color}
\usepackage[dvipsnames]{xcolor}

%tikz
\usepackage{tikz}
\usetikzlibrary{arrows,arrows.meta,intersections, calc,positioning,decorations.pathreplacing,decorations.pathmorphing,shapes}
\usetikzlibrary{patterns}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{knots}





%%%%%%%%%%%%%%%%%%%%%%% mathcal %%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\CA}{\mathcal{A}}
\newcommand{\CB}{\mathcal{B}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\CD}{\mathcal{D}}
\newcommand{\CE}{\mathcal{E}}
\newcommand{\CF}{\mathcal{F}}
\newcommand{\CG}{\mathcal{G}}
\newcommand{\CH}{\mathcal{H}}
\newcommand{\CI}{\mathcal{I}}
\newcommand{\CJ}{\mathcal{J}}
\newcommand{\CK}{\mathcal{K}}
\newcommand{\CL}{\mathcal{L}}
\newcommand{\CM}{\mathcal{M}}
\newcommand{\CN}{\mathcal{N}}
\newcommand{\CO}{\mathcal{O}}
\newcommand{\CP}{\mathcal{P}}
\newcommand{\CQ}{\mathcal{Q}}
\newcommand{\CR}{\mathcal{R}}
\newcommand{\CS}{\mathcal{S}}
\newcommand{\CT}{\mathcal{T}}
\newcommand{\CU}{\mathcal{U}}
\newcommand{\CV}{\mathcal{V}}
\newcommand{\CW}{\mathcal{W}}
\newcommand{\CX}{\mathcal{X}}
\newcommand{\CY}{\mathcal{Y}}
\newcommand{\CZ}{\mathcal{Z}}


%%%%%%%%%%%%%%%%%  notations  %%%%%%%%%%%%%%%%%%

\newcommand{\inn}[2]{\langle#1,#2\rangle}%inner product
\newcommand{\norm}[1]{\|#1\|}%norm
\newcommand{\N}{\mathbb{N}}%natural number
\newcommand{\R}{\mathbb{R}}%real number
\newcommand{\C}{\mathbb{C}}%complex number
\newcommand{\Q}{\mathbb{Q}}%rational number
\newcommand{\Z}{\mathbb{Z}}%integer
\newcommand{\imineq}[2]{\vcenter{\hbox{\includegraphics[height=#2ex]{#1}}}}
\newcommand{\BS}{\mathbb{S}}
\newcommand{\BB}{\mathbb{B}}
\renewcommand{\d}{\mathrm{d}}
\renewcommand{\Set}[2]{\left\{#1\;\middle|\;#2\right\}}




\newcommand{\Smat}[2]{{\mathcal{S}_{#1}}^{#2}}
\newcommand{\Sdag}[2]{{\left(\mathcal{S}^{\dagger}\right)_{#1}}^{#2}}
\newcommand{\Tdag}[2]{{\left(\mathcal{T}^{\dagger}\right)_{#1}}^{#2}}
\newcommand{\Tmat}[2]{{\mathcal{T}_{#1}}^{#2}}
\newcommand{\fusion}[3]{{\mathcal{N}_{#1#2}}^{#3}}
\newcommand{\sph}{\mathbb{S}}
\newcommand{\torus}{\mathbb{T}}
\newcommand{\ball}{\mathbb{B}}
\newcommand{\disk}{\mathbb{D}^2}
\newcommand{\lie}[1]{\mathfrak{#1}}
\newcommand{\aff}[1]{\widehat{\mathfrak{#1}}} %affine lie algebra
\newcommand{\rep}[1]{\mathbf{#1}}
\newcommand{\nc}{G_{\text{N}}} %Newton constant
\newcommand{\PI}[1]{\mathcal{D}#1\,}
\newcommand{\tm}[2]{\tau^{#1|#2}} %transition matrix
\newcommand{\ttm}[2]{\widetilde{\tau}^{#1|#2}} %unnormalized transition matrix
\newcommand{\vir}{\mathrm{Vir}} %Virasoro algebra





%%%%%%%%%%%%%%%%%% math operators %%%%%%%%%%%%%%%%%%%%%%%%
% \newcommand{\tr}{\mathrm{tr}}%tr
% \newcommand{\Tr}{\mathrm{Tr}}%Tr
% \renewcommand{\d}{\mathrm{d}}
% \newcommand{\ad}{\mathrm{ad}}
% \newcommand{\Ad}{\mathrm{Ad}}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\ad}{ad}
\DeclareMathOperator{\Ad}{Ad}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}

\let\Re\relax
\DeclareMathOperator{\Re}{Re}
\let\Im\relax
\DeclareMathOperator{\Im}{Im}

%%%%%%%%%%%%%%%%%%%%%% comment %%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\cmt}[1]{\textcolor{red}{\textbf{(#1)}}}





%%%%%%%%%%%%%%%%%%%%%% 数式番号 %%%%%%%%%%%%%%%%%%%

\makeatletter
\renewcommand{\theequation}{%
\thesection.\arabic{equation}}
\@addtoreset{equation}{chapter}
\makeatother

%%%%%%%%%%%%%%%%%%%%%% 証明環境 %%%%%%%%%%%%%%%%%%%
\makeatletter
\renewenvironment{proof}[1][証明]{\par
  \pushQED{\qed}%
  \normalfont \topsep6\p@\@plus6\p@\relax
  \trivlist
  \item\relax
  {\bfseries
  #1\@addpunct{.}}\hspace\labelsep\ignorespaces
}{%
  \popQED\endtrivlist\@endpefalse
}
\makeatother


\setcounter{tocdepth}{3} % subsubsection まで目次に表示



\begin{document}

\tableofcontents

\section{重要な定理まとめ}
\subsection{推定論}
$X$をパラメータ$\theta$を持つある確率分布に従う確率変数であるとする。Fisher情報量$I(\theta)$を
\begin{align}
        I(\theta) = \E_\theta\left[\left(\frac{\partial \log f(X|\theta)}{\partial \theta}\right)^2\right]
\end{align}
と定義する。期待値$\E_\theta[\cdot]$と$\theta$微分の交換に関する正則条件を満たす限り、Fisher情報量を
\begin{align}
    I(\theta) = -\E_\theta\left[\frac{\partial^2 \log f(X|\theta)}{\partial \theta^2}\right]
\end{align}
で定義しても等価である。後者の形式の方が計算しやすい場合も多い。

なお、上記の定義において$f(X|\theta)$となっているところを$n$サンプル$X_1,\ldots,X_n$の尤度関数$L(\theta|X_1,\ldots,X_n)=\prod_{i=1}^nf(X_i|\theta)$に置き換えて定義したものを$I_n(\theta)$と書くと、$X_1, \ldots, X_n$が独立同分布である限り$I_n(\theta)=nI(\theta)$が成り立つ。
$I(\theta)$と$I_n(\theta)$のどちらをFisher情報量と呼ぶのかは分野や文献に依る。
ここでは$I(\theta)$をFisher情報量と呼ぶことにし、混合を避けるためにできるだけ$I(\theta)$のみを用いることにする。

以下では$n$個の独立同分布$\bm{X}=(X_1, \ldots,X_n)^{\mathrm{T}}$の実現値からパラメータ$\theta$の値を推定する問題を考える。推定量は$\widehat{\theta}(\bm{X})$のようにハットをつけて表し、引数の$\bm{X}$はしばしば省略する。

\begin{theorem}
    推定量$\widehat{\theta}$について成り立つ以下の不等式を、Cram\'{e}r-Raoの不等式という。
    \begin{enumerate}[(1)]
        \item $\widehat{\theta}$が不偏推定量であるとき、
        \begin{align}
            \Var_\theta[\widehat{\theta}] \ge \frac{1}{nI(\theta)}
        \end{align}
        が成り立つ。
        \item $\widehat{\theta}$が不偏推定量でないときは、バイアスを$b(\theta):=\E_\theta[\widehat{\theta}]-\theta$とすると、
        \begin{align}
            \Var_\theta[\widehat{\theta}]\ge\frac{(1 + b'(\theta))^2}{nI(\theta)}
        \end{align}
        が成り立つ。
    \end{enumerate}
\end{theorem}

\begin{theorem}\label{th:UMVU1}
    $\widehat{\theta}$が不偏推定量であるとする。$\widehat{\theta}$がCram\'{e}r-Raoの不等式の下限
    \begin{align}\label{CRinf}
        \Var_\theta[\widehat{\theta}] = \frac{1}{nI(\theta)}
    \end{align}
    を満たすならば、$\widehat{\theta}$は一様最小分散不偏（UMVU）推定量である。
\end{theorem}

このCram\'{e}r-Raoの下限\eqref{CRinf}を用いたUMVUの判定方法は、あくまで十分条件を与えるのみであることに注意。一方、以下の完全十分統計量による判定方法はUMVU推定量の完全な構成法を与える。

\begin{theorem}\label{th:UMVU2}
    $T$が完全十分統計量であるとき、以下の2つが成り立つ。
    \begin{enumerate}[(1)]
        \item $T$の関数である不偏推定量$\delta(T)$は一意に定まり、$\delta(T)$はUMVU推定量である。
        \item 任意の不偏推定量$\widehat{\theta}$に対して、$\E[\widehat{\theta}|T]$は$\delta(T)$に一致する。
    \end{enumerate}
\end{theorem}

この定理は、十分統計量に関する重要な定理であるRao-Blackwellの定理を完備十分統計量に対して適用したときの一つの帰結である。

\subsection{検定論}
検定論では、一般に帰無仮説$H_0:\theta\in \Theta_0$と対立仮説$H_1:\theta\in\Theta_1$を考える。ここで$\Theta_0$と$\Theta_1$は$\Theta_0\cap\Theta_1=\emptyset$を満たし、$\Theta_0\cup\Theta_1$はパラメータ空間全体$\Theta$と一致するような集合である。

確率変数$X$の観測値に基づく決定関数$\delta(X)$を、
\begin{align}
    \delta(X) = 
    \begin{cases}
        0 & \text{帰無仮説を受容するとき} \\
        1 & \text{帰無仮説を棄却するとき}
    \end{cases}
\end{align}
となるような関数として定義する。帰無仮説が正しいときに帰無仮説を棄却する誤り（$\delta(X)=1$ when $\theta\in\Theta_0$）を第一種の過誤、対立仮説が正しいときに帰無仮説を受容する誤り（$\delta(X)=0$ when $\theta\in\Theta_1$）を第二種の過誤という。
第一種の過誤と第二種の過誤の間にはトレードオフがあることが知られている。
つまり、第一種の過誤を小さくするほど第二種の過誤が大きくなり、第二種の過誤を小さくするほど第一種の過誤が大きくなる。
そこで検定論でよく用いられる手法は、まず第一種の過誤の確率を一定の値$\alpha$以下に抑えながら、可能な限り第二種の過誤を小さくするような決定関数$\delta$を求める、というものである。ここで$\alpha$は有意水準と呼ばれる。
このような検定論の手法をNeyman-Pearsonの基準という。

ここで、検出力関数$\beta_\delta(\theta)$を
\begin{align}
    \beta_\delta(\theta) := P_\theta(\delta(X)=1)
\end{align}
で定義する。検出力関数を用いると、第一種・第二種の過誤の確率を以下のように表すことができる。
\begin{itemize}
    \item $\theta\in \Theta_0$のとき、$\beta_\delta(\theta)$は第一種の過誤の確率を表す。（このときの$\beta_\delta(\theta)$をサイズという。）
    \item $\theta\in\Theta_1$のとき、$1-\beta_\delta(\theta)$は第二種の過誤の確率を表す。（このときの$\beta_\delta(\theta)$をパワーという。）
\end{itemize}
よって、検出力関数を用いて検定論を定式化すると、「$\theta\in\Theta_0$について$\beta_\delta(\theta)\le\alpha$を満たしながら$\theta\in\Theta_1$について$\beta_\delta(\theta)$ができるだけ大きくなるような決定関数$\delta$を求める問題」となる。

なお、検出力関数の記法については分野や文献によってブレがあり、ここでの$1-\beta_\delta(\theta)$に相当するものを$\beta_{\delta}(\theta)$のように表記する場合もあるので注意。

検定の最適性に関して、検定問題の状況に応じていくつかの概念が存在する。
単純対立仮説$\Theta_1=\{\theta_1\}$の場合、有意水準$\alpha$の検定の中で$\beta_\delta(\theta_1)$が最大となる$\delta$を最強力（Most Powerful, MP）検定という。
単純対立仮説ではない場合、任意の$\theta\in\Theta_1$に対して$\beta_\delta(\theta)$が最大になるような$\delta$を一様最強力（Uniformly Most Powerful, UMP）検定という。
一様最強力検定の条件はかなり厳しいため、存在しない場合も多い。
そのような場合に考えられる検定のクラスの一つとして、不偏検定という概念がある。
有意水準$\alpha$の検定$\delta$が不偏であるとは、任意の$\theta\in\Theta_1$に対して$\beta_\delta(\theta)\ge\alpha$を満たすことをいう。
このように定義された不偏検定の中で、検出力関数$\beta_\delta(\theta)$が任意の$\theta\in\Theta_1$に対して最大となるような検定を一様最強力不偏（Uniformly Most Powerful Unbiased, UMPU）検定という。
UMPU検定では比較する検定のクラスが不偏検定に制限されるため、UMP検定に比べて条件が緩く、存在しやすい。

検定問題には様々なクラスのものがあるが、ここでは以下の3種類の問題を考える：
\begin{itemize}
    \item 単純帰無仮説と単純対立仮説の間の検定 $H_0: \theta=\theta_0 \ \text{v.s.}\  H_1: \theta=\theta_1$
    \item 片側検定 $H_0: \theta\le\theta_0\ \text{v.s.}\ \theta>\theta_0$
    \item 両側検定 $H_0: \theta=\theta_0\ \text{v.s.}\ \theta\neq \theta_0$
\end{itemize}
これらの検定問題を解くために有用な定理を以下で述べる。

\begin{theorem}[Neyman-Pearsonの補題]\label{th:NeymanPearson}
    有意水準を$\alpha$とする。検定問題$H_0: \theta=\theta_0 \ \text{v.s.}\  H_1: \theta=\theta_1$を考える。与えられた$c\ge0$と$r\in[0, 1]$に対して、検定関数
    \begin{align}
        \delta_{c, r}(x) = 
        \begin{dcases}
            1, & \text{if}\quad \frac{f(x|\theta_1)}{f(x|\theta_0)}>c \\
            r, & \text{if}\quad \frac{f(x|\theta_1)}{f(x|\theta_0)}=c \\
            0, & \text{if}\quad \frac{f(x|\theta_1)}{f(x|\theta_0)}<c
        \end{dcases}
    \end{align}
    を考え、$c, r$として$P_{\theta_0}(\delta_{c, r}(X)=1)=\alpha$を満たすものをとる。このとき、$\delta_{c, r}$は有意水準$\alpha$の検定の中で最強力検定となる。
\end{theorem}

この定理は、単純帰無仮説と単純対立仮説の間の検定については尤度比検定が最強力検定になることを示している。$f(x|\theta_i)$は連続分布の場合はpdfで、離散分布の場合はpmfであるが、連続の場合は$r$を用いた確率化は不要。

次に片側検定を考える。この場合は対数尤度を、Neyman-Pearsonの補題を直接用いることはできないが、$f(x|\theta)$が特定の条件を満たす場合はNeyman-Pearsonの補題を拡張することができる。

\begin{theorem}[Karlin-Rubinの定理]
    有意水準を$\alpha$とする片側検定$H_0: \theta\le\theta_0\ \text{v.s.}\ \theta>\theta_0$を考える。尤度比がある統計量$T(x)$を用いて
    \begin{align}
        \frac{f(x|\theta_2)}{f(x|\theta_1)}=g(T(x), \theta_1, \theta_2)
    \end{align}
    の形で書け、この関数が任意の$\theta_1, \theta_2\  (\theta_1<\theta_2)$に対して、$T(x)$の単調増加関数になっているとする。与えられた$c\in\R$と$r\in[0, 1]$に対して、検定関数
    \begin{align}
        \delta_{c, r}(x) = 
        \begin{dcases}
            1, & \text{if}\quad T(x)>c \\
            r, & \text{if}\quad T(x)=c \\
            0, & \text{if}\quad T(x)<c
        \end{dcases}
    \end{align}
    を考え、$c, r$として$P_{\theta_0}(\delta_{c, r}(X)=1)=\alpha$を満たすものをとる。このとき、$\delta_{c, r}$は有意水準$\alpha$の検定の一様最強力検定となる。
\end{theorem}


\subsection{区間推定}
パラメータ$\theta\in\Theta$を持つ確率分布に従う確率変数$X$を考える。$\Theta$の部分集合$S(X)$が信頼係数$1-\alpha$の信頼域であるとは、
\begin{align}\label{confidence}
    P_\theta(\theta\in S(X))\ge 1-\alpha\,,\qquad \forall\theta\in\Theta
\end{align}
を満たすことをいう。信頼域$S(X)$を推定することを、区間推定という。

信頼域$S(X)$の定義\eqref{confidence}の意味を言葉で表現すると、「『真のパラメータが$\theta$であるときに$\theta$が$S(X)$に含まれる確率が$1-\alpha$以上』がどんな$\theta$に対しても成り立つこと」である。つまり、区間推定とは、真のパラメータが高い確率（$\ge 1-\alpha$）で含まれるような範囲を確率変数の観測値から推定することである。\cmt{この説明は適切ではない？}

信頼域$S(X)$は、仮説検定を用いて以下の定理から構築することができる。
\begin{theorem}\label{th:confidenceregion}
    任意の$\theta_0$に対して、$\theta = \theta_0$を単純帰無仮説とするような両側検定
    \begin{align}
            H_0: \theta=\theta_0,\qquad H_1: \theta\neq \theta_0
    \end{align}
    を考える。$A(\theta_0)$を、この検定における有意水準$\alpha$の受容域とする。すなわち、
    \begin{align}
        P_{\theta_0}(X\in A(\theta_0))\ge 1-\alpha
    \end{align}
    を満たすとする。このとき、パラメータ空間の部分集合$S(X)\in\Theta$を
    \begin{align}
        S(X) = \left\{\theta\mid x\in A(\theta)\right\}
    \end{align}
    と定義すると、$S(X)$は信頼係数$1-\alpha$の信頼域である。
\end{theorem}


\section{正規分布}
本節では、正規分布に関する推定量の基本的事項についてのまとめ・導出を行う。

実数値の確率変数$X$がパラメータ$\mu, \sigma$をもつ正規分布の確率密度関数
\begin{align}
    f(x|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)
\end{align}
に従うとき、$X\sim N(\mu, \sigma^2)$と表記する。

\subsection{1標本問題}
$X_1,\ldots,X_n \sim N(\mu, \sigma^2)$, i.i.d. とする。また、$\bar{X} := \frac{1}{n}\sum_{i=1}^n X_i$とする。このように、1つの母集団に関する推測問題を1標本問題と呼ぶ。
\subsubsection{点推定}
\begin{proposition}
    パラメータ$\mu,\sigma^2$の推定量に関して、以下が成り立つ。
    \begin{enumerate}[(1)]
        \item $\sigma^2$が既知か未知かにかかわらず
        \begin{align}
            \widehat{\mu} := \bar{X} = \frac{1}{n}\sum_{i=1}^n X_i
        \end{align}
        は$\mu$のUMVU推定量かつ最尤推定量である。
        \item $\mu$が既知の場合、
        \begin{align}
            \widehat{\sigma^2} :=\frac{1}{n}\sum_{i=1}^n(X_i - \mu)^2
        \end{align}
        は$\sigma^2$のUMVU推定量かつ最尤推定量である。
        \item $\mu$が未知の場合、$\sigma^2$のUMVU推定量および最尤推定量はそれぞれ
        \begin{align}
            \widehat{\sigma^2}_{\text{unbiased}} :=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X})^2\, , \qquad \widehat{\sigma^2}_{\text{MLE}} :=\frac{1}{n}\sum_{i=1}^n(X_i-\bar{X})^2
        \end{align}
        で与えられる。
    \end{enumerate}
\end{proposition}
\begin{proof}
    \begin{enumerate}[(1)]
        \item $\E[\widehat{\mu}]=\mu$が成り立つから、$\widehat{\mu}$は$\mu$の不偏推定量。
        \begin{align}
            \log f(X|\mu,\sigma^2) = -\frac{1}{2}\log(2\pi\sigma^2) - \frac{(X-\mu)^2}{2\sigma^2}
        \end{align}
        より、$\mu$に関するFisher情報量は
        \begin{align}
            I(\mu) = \E\left[\left(\frac{X-\mu}{\sigma^2}\right)^2\right] = \frac{1}{\sigma^2}\, .
        \end{align}
        $X_1,\ldots,X_n$は互いに独立だから、
        \begin{align}
            \Var[\widehat{\mu}] = \frac{1}{n^2}\sum_{i=1}^n\Var[X_i] = \sigma^2/n\, .
        \end{align}
        よって定理\ref{th:UMVU1}より$\widehat{\mu}$はUMVU推定量である。

        次に最尤推定量が$\widehat{\mu}$になることを示す。対数尤度は、
        \begin{align}\label{llnorm}
            \ell(\mu, \sigma^2 | \bm{X}) = -\frac{n}{2}\log(2\pi\sigma^2) - \sum_{i=1}^n\frac{(X_i-\mu)^2}{2\sigma^2}
        \end{align}
        これを$\mu$について最大化すると、
        \begin{align}\label{muMLEnorm}
            \widehat{\mu}_{\text{MLE}} = \frac{1}{n}\sum_{i=1}^n X_i
        \end{align}
        となり、これは$\widehat{\mu}$に一致。

        \item $X_1,\ldots,X_n$は互いに独立であることから$\E[\widehat{\sigma^2}]=\sigma^2$が成り立つため、$\widehat{\sigma^2}$は$\sigma^2$の不偏推定量。$\tau=\sigma^2$とおくと
        \begin{align}
            \frac{\partial^2}{\partial \tau^2}\log f(X|\mu, \tau)=\frac{1}{2\tau^2}-\frac{(X-\mu)^2}{\tau^3}
        \end{align}
        となり、$\tau$に関するFisher情報量は
        \begin{align}
            I(\tau) = -\frac{1}{2\tau^2} + \frac{1}{\tau^2} = \frac{1}{2\tau^2}\,.
        \end{align}
        一方、$\widehat{\sigma^2}$の分散は
        \begin{align}
            \Var[\widehat{\sigma^2}] = \frac{1}{n^2}\sum_{i=1}^n \Var[(X_i-\mu)^2]=\frac{2\sigma^4}{n}
        \end{align}
        である。最後の等号は$\left((X_i-\mu)/\sigma\right)^2$が自由度1の$\chi^2$分布に従うことから（$\chi^2$分布の分散を知っていれば）求めることができる（当然定義通りに計算してもよいが、4次モーメントを求める必要があるためやや面倒）。これらの結果と定理\ref{th:UMVU1}より$\widehat{\sigma^2}$はUMVU推定量である。
        次に最尤推定量を求める。\eqref{llnorm}より
        \begin{align}
            \frac{\partial}{\partial \tau}\ell(\mu, \tau|\bm{X}) = -\frac{n}{2\tau}+\sum_{i=1}^n\frac{(X_i-\mu)^2}{2\tau^2}\,.
        \end{align}
        よって最尤推定量は
        \begin{align}\label{sigmaMLEnorm}
            \widehat{\sigma^2}_{\text{MLE}} = \frac{1}{n}\sum_{i=1}^n(X_i-\mu)^2
        \end{align}
        となり、$\widehat{\sigma^2}$に一致する。

        \item 分散の定義が平均値$\mu$に陽に依存するため、$\mu$が既知か未知かによって推定量が変わり得る。
        \begin{align}
            \begin{aligned}
            \E\left[\sum_{i=1}^n(X_i-\bar{X})^2\right] &= \sum_{i=1}^n\E\left[(X_i-\mu  + \mu - \bar{X})^2\right] \\
            & = \sum_{i=1}^n\E[(X_i-\mu)^2] -2\E\left[\sum_{i=1}^n(X_i-\mu)(\bar{X}-\mu)\right] + \sum_{i=1}^n\E\left[(\bar{X}-\mu)^2\right]\\
            & = n\sigma^2 -2n\E[(\bar{X}-\mu)^2] + \E[(\bar{X}-\mu)^2]\\
            & = n\sigma^2 - n\Var(\bar{X})\\
            & = (n-1) \sigma^2
            \end{aligned}
        \end{align}
        より、$\sigma^2$の不偏推定量は
        \begin{align}
            \widehat{\sigma^2}_{\text{unbiased}} =\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X})^2
        \end{align}
        のように、$n$の代わりに$n-1$で割ったものとなる。ここでは実際に確かめることはしないが、これはCram\'{e}r-Raoの不等式の下限を満たさない。よってこれが実際にUMVU推定量であることを示すためには定理\ref{th:UMVU2}を使う必要がある。
        $X_1,\ldots,X_n$の同時密度分布が
        \begin{align*}
            f(\bm{x}) & = \frac{1}{(2\pi\sigma^2)^{n/2}}\prod_{i=1}^n\exp\left(-\frac{(x_i-\mu)^2}{2\sigma^2}\right) \\
            & =\frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left(-\frac{n(\bar{x}-\mu)^2}{2\sigma^2}-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\bar{x})^2\right)
        \end{align*}
        と書けることから、$T=(\bar{X}, \sum_{i=1}^n(X_i-\bar{X})^2)$は十分統計量であり、証明は省略するがこれは完備である。以上のことから、$\widehat{\sigma^2}_{\text{unbiased}}$は不偏推定量かつ完備十分統計量（の関数）であるから、定理\ref{th:UMVU2}よりUMVU推定量である。

        次に最尤推定量を求める。この場合は対数尤度を$\mu$と$\sigma$で同時に最大化する必要がある。しかし、(1)で求めた$\mu$の最尤推定量\eqref{muMLEnorm}は$\sigma$に依らないため今の場合も同様に成り立ち、その$\widehat{\mu}_{\text{MLE}}$を$\mu$に代入して$\sigma$について最大化すればよい。当然その結果は(2)の結果\eqref{sigmaMLEnorm}の$\mu$を$\widehat{\mu}_{\text{MLE}}$で置き換えたものになるため、
        \begin{align}
            \widehat{\sigma^2}_{\text{MLE}} = \frac{1}{n}\sum_{i=1}^n(X_i-\bar{X})^2
        \end{align}
        となる。
    \end{enumerate}
\end{proof}

\subsubsection{検定}



\subsubsection{区間推定}
\begin{theorem}
    有意水準を$\alpha$とする。$\mu$の信頼区間は、以下で与えられる。
    \begin{enumerate}[(1)]
        \item $\sigma^2$が既知の場合、
        \begin{align}
            \bar{X} \pm z_{\alpha/2}\cdot\frac{\sigma}{\sqrt{n}}\,.
        \end{align}
        ここで、$z_{\alpha}$は標準正規分布$N(0, 1)$の両側$\alpha$点である。また、これは一様最強力不偏信頼区間である。

        \item $\sigma^2$が未知の場合、
        \begin{align}
            \bar{X} \pm t_{\alpha/2}(n-1)\cdot\frac{s}{\sqrt{n}}\,.
        \end{align}
        ここで、$t_{\alpha/2}(n-1)$は自由度$n-1$の$t$分布の両側$\alpha$点であり、$s$は不偏分散
        \begin{align}\label{sqrtubs}
            s^2 = \frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X})^2
        \end{align}
        の平方根である。また、これは一様最強力不偏信頼区間である。
    \end{enumerate}
\end{theorem}

\begin{proof}
    \begin{enumerate}[(1)]
        \item 任意の$\mu_0$に対して、$\mu=\mu_0$を単純帰無仮説とするような両側検定
        \begin{align}\label{mutest1}
            H_0: \mu=\mu_0,\qquad H_1: \mu\neq \mu_0
        \end{align}
        を考える。帰無仮説のもとで、$\bar{X}\sim N(\mu_0, \sigma^2/n)$より、
        \begin{align}
            \frac{\bar{X}-\mu_0}{\sigma/\sqrt{n}}
        \end{align}
        は標準正規分布に従うから、
        \begin{align}
            -z_{\alpha/2} < \frac{\bar{X}-\mu_0}{\sigma/\sqrt{n}} < z_{\alpha/2}
        \end{align}
        は検定\eqref{mutest1}の有意水準$1-\alpha$の受容域である。定理\ref{th:confidenceregion}より、この不等式を一般の$\mu$について解いたもの
        \begin{align}
            \bar{X} - z_{\alpha/2}\cdot\frac{\sigma}{\sqrt{n}} < \mu < \bar{X} + z_{\alpha/2}\cdot\frac{\sigma}{\sqrt{n}}
        \end{align}
        は$\mu$の信頼域を与える。\cmt{一様最強力不偏信頼区間の証明}

        \item 任意の$\mu_0$に対して、$\mu=\mu_0$を単純帰無仮説とするような両側検定
        \begin{align}\label{mutest2}
            H_0: \mu=\mu_0,\qquad H_1: \mu\neq \mu_0
        \end{align}
        を考える。$\sigma^2$が未知の場合は(1)のときのように$\bar{X}$が正規分布に従うと言えない。帰無仮説のもとで、\eqref{sqrtubs}で定義された$s$を用いて
        \begin{align}
            T = \frac{\sqrt{n}(\bar{X}-\mu_0)}{s}
        \end{align}
        と定義された$T$は$t$統計量と呼ばれ、自由度$n-1$の$t$分布に従うことが知られている。よって、
        \begin{align}
            -t_{\alpha/2}(n-1) < \frac{\sqrt{n}(\bar{X}-\mu_0)}{s} < t_{\alpha/2}
        \end{align}
        は検定\eqref{mutest2}の有意水準$1-\alpha$の受容域である。定理\ref{th:confidenceregion}より、この不等式を一般の$\mu$について解いたもの
        \begin{align}
            \bar{X} - t_{\alpha/2}(n-1)\cdot\frac{s}{\sqrt{n}} < \mu < \bar{X} + t_{\alpha/2}(n-1)\cdot\frac{s}{\sqrt{n}}
        \end{align}
        は$\mu$の信頼域を与える。\cmt{一様最強力不偏信頼区間の証明}
    \end{enumerate}
\end{proof}

\begin{theorem}
    有意水準を$\alpha$とする。$\sigma^2$の信頼区間は
    \begin{align}
        \frac{(n-1)s^2}{\chi^2_{\alpha/2}(n-1)} < \sigma^2 < \frac{(n-1)s^2}{\chi^2_{1-\alpha/2}(n-1)}
    \end{align}
    となる。ここで、$s^2$は不偏分散である。
\end{theorem}
\begin{proof}
    任意の$\sigma_0^2$に対して、$\sigma^2=\sigma^2_0$を単純帰無仮説とするような両側検定
    \begin{align}\label{sigmatest}
            H_0: \sigma^2=\sigma^2_0,\qquad H_1: \sigma^2\neq \sigma^2_0
    \end{align}
    を考える。帰無仮説のもとで、$(n-1)s^2/\sigma_0^2$は自由度$n-1$の$\chi^2$分布に従うため、
    \begin{align}
        \chi_{1-\alpha/2}^2(n-1) < \frac{(n-1)s^2}{\sigma_0^2} < \chi_{\alpha/2}^2(n-1)
    \end{align}
    は有意水準$1-\alpha$の受容域である。定理\ref{th:confidenceregion}より、この不等式を一般の$\sigma^2$について解いたもの
    \begin{align}
        \frac{(n-1)s^2}{\chi^2_{\alpha/2}(n-1)} < \sigma^2 < \frac{(n-1)s^2}{\chi^2_{1-\alpha/2}(n-1)}
    \end{align}
    は$\sigma^2$の信頼域を与える。
\end{proof}

\section{Bernoulli分布・二項分布}
\subsection{1標本問題}
まずは1標本問題を考える。つまり、$X_1,\ldots, X_n\sim \text{Bernoulli}(p)$, i.i.d. とする。Bernoulli分布の期待値は$\E[X_i]=p$、分散は$\Var[X_i]=p(1-p)$である。$n$サンプルの和$S_n=\sum_{i=1}^nX_i$は二項分布$\text{Bin}(n, p)$に従い、$\E[S_n]=np$、$\Var(S_n)=np(1-p)$である。
\subsubsection{点推定}
\begin{theorem}
    パラメータ$p$の推定量に関して、
        \begin{align}
            \widehat{p} := \frac{1}{n}\sum_{i=1}^n X_i
        \end{align}
        は$p$のUMVU推定量かつ最尤推定量である。
\end{theorem}
\begin{proof}
    $\E[\widehat{p}]=p$が成り立つから、$\widehat{p}$は$p$の不偏推定量。
    \begin{align}
        f(x|p) = p^x(1-p)^{1-x},\quad x=0, 1
    \end{align}
    より、$p$に関するFisher情報量は
    \begin{align}
        I(p) = \E\left[\frac{X}{p^2}+\frac{1-X}{(1-p)^2}\right] = \frac{1}{p(1-p)}\, .
    \end{align}
    $X_1,\ldots,X_n$は互いに独立だから、
    \begin{align}
        \Var[\widehat{p}] = \frac{1}{n^2}\sum_{i=1}^n\Var[X_i] = \frac{p(1-p)}{n}\, .
    \end{align}
    よって定理\ref{th:UMVU1}より$\widehat{p}$はUMVU推定量である。

    次に最尤推定量が$\widehat{p}$になることを示す。対数尤度は、
    \begin{align}
        \ell(p|\bm{X}) = \sum_{i=1}^n\left(X_i\log p + (1-X_i)\log(1-p)\right)
    \end{align}
    これを$p$について最大化すると、
    \begin{align}
        \widehat{p}_{\text{MLE}} = \frac{1}{n}\sum_{i=1}^n X_i
    \end{align}
    となり、これは$\widehat{p}$に一致。
\end{proof}

\subsubsection{検定}
\subsubsection{区間推定}

\end{document}


